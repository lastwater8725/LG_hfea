{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2682242-ba9a-4218-a95f-f436b7f5f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4adb862-072f-4629-b517-344740dc5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv') \n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5712b288-7e69-4a94-beb8-89be8d403928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임신 시도 또는 마지막 임신 경과 연수    246981\n",
      "특정 시술 유형                      2\n",
      "단일 배아 이식 여부                6291\n",
      "착상 전 유전 검사 사용 여부         253633\n",
      "착상 전 유전 진단 사용 여부           6291\n",
      "배아 생성 주요 이유                6291\n",
      "총 생성 배아 수                  6291\n",
      "미세주입된 난자 수                 6291\n",
      "미세주입에서 생성된 배아 수            6291\n",
      "이식된 배아 수                   6291\n",
      "미세주입 배아 이식 수               6291\n",
      "저장된 배아 수                   6291\n",
      "미세주입 후 저장된 배아 수            6291\n",
      "해동된 배아 수                   6291\n",
      "해동 난자 수                    6291\n",
      "수집된 신선 난자 수                6291\n",
      "저장된 신선 난자 수                6291\n",
      "혼합된 난자 수                   6291\n",
      "파트너 정자와 혼합된 난자 수           6291\n",
      "기증자 정자와 혼합된 난자 수           6291\n",
      "동결 배아 사용 여부                6291\n",
      "신선 배아 사용 여부                6291\n",
      "기증 배아 사용 여부                6291\n",
      "대리모 여부                     6291\n",
      "PGD 시술 여부                254172\n",
      "PGS 시술 여부                254422\n",
      "난자 채취 경과일                 57488\n",
      "난자 해동 경과일                254915\n",
      "난자 혼합 경과일                 53735\n",
      "배아 이식 경과일                 43566\n",
      "배아 해동 경과일                215982\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치가 존재하는 열과 개수 출력\n",
    "train_missing_values = train.isnull().sum()\n",
    "train_missing_columns = train_missing_values[train_missing_values > 0]  # 결측치가 있는 열만 필터링\n",
    "\n",
    "print(train_missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59e84b5-3571-4430-afd9-e632a575fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임신 시도 또는 마지막 임신 경과 연수    86770\n",
      "단일 배아 이식 여부               2176\n",
      "착상 전 유전 검사 사용 여부         89134\n",
      "착상 전 유전 진단 사용 여부          2176\n",
      "배아 생성 주요 이유               2176\n",
      "총 생성 배아 수                 2176\n",
      "미세주입된 난자 수                2176\n",
      "미세주입에서 생성된 배아 수           2176\n",
      "이식된 배아 수                  2176\n",
      "미세주입 배아 이식 수              2176\n",
      "저장된 배아 수                  2176\n",
      "미세주입 후 저장된 배아 수           2176\n",
      "해동된 배아 수                  2176\n",
      "해동 난자 수                   2176\n",
      "수집된 신선 난자 수               2176\n",
      "저장된 신선 난자 수               2176\n",
      "혼합된 난자 수                  2176\n",
      "파트너 정자와 혼합된 난자 수          2176\n",
      "기증자 정자와 혼합된 난자 수          2176\n",
      "동결 배아 사용 여부               2176\n",
      "신선 배아 사용 여부               2176\n",
      "기증 배아 사용 여부               2176\n",
      "대리모 여부                    2176\n",
      "PGD 시술 여부                89286\n",
      "PGS 시술 여부                89396\n",
      "난자 채취 경과일                19949\n",
      "난자 해동 경과일                89575\n",
      "난자 혼합 경과일                18579\n",
      "배아 이식 경과일                15246\n",
      "배아 해동 경과일                76117\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치가 존재하는 열과 개수 출력\n",
    "test_missing_values = test.isnull().sum()\n",
    "test_missing_columns = test_missing_values[test_missing_values > 0]  # 결측치가 있는 열만 필터링\n",
    "\n",
    "print(test_missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a616bed6-a4bb-4d06-9f2f-7eae164cff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['임신 시도 또는 마지막 임신 경과 연수'], inplace=True)  \n",
    "test.drop(columns=['임신 시도 또는 마지막 임신 경과 연수'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb55c2e-e380-47c6-8b81-1e065a017269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\814923478.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train['특정 시술 유형'].fillna('UnKnown', inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\814923478.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test['특정 시술 유형'].fillna('UnKnown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train['특정 시술 유형'].fillna('UnKnown', inplace=True)\n",
    "test['특정 시술 유형'].fillna('UnKnown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30068a7c-89e8-401b-8b03-29a8d69b59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['착상 전 유전 검사 사용 여부'], inplace=True)  \n",
    "test.drop(columns=['착상 전 유전 검사 사용 여부'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7e144c-f40f-420c-8400-fd4425b159ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['PGD 시술 여부'], inplace=True)  \n",
    "test.drop(columns=['PGD 시술 여부'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236e8e4d-71c0-44d5-a64c-50fe6e430927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['PGS 시술 여부'], inplace=True)  \n",
    "test.drop(columns=['PGS 시술 여부'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47962843-8813-4440-8af1-f4850993461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['난자 해동 경과일'], inplace=True) \n",
    "test.drop(columns=['난자 해동 경과일'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a366f8ee-5ab9-4ee2-8968-97fe69f6d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['배아 해동 경과일'], inplace=True)  \n",
    "test.drop(columns=['배아 해동 경과일'], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192a5f42-5e32-43cf-ae17-e22068f8c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['난자 채취 경과일'] = train['난자 채취 경과일'].fillna(train['난자 채취 경과일'].mode()[0])\n",
    "test['난자 채취 경과일'] = test['난자 채취 경과일'].fillna(train['난자 채취 경과일'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb60547b-4724-43db-ba6e-8d9de9e142a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['난자 혼합 경과일'] = train['난자 혼합 경과일'].fillna(train['난자 혼합 경과일'].mode()[0])\n",
    "test['난자 혼합 경과일'] = test['난자 혼합 경과일'].fillna(train['난자 혼합 경과일'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d37b9e-cad1-468b-831b-e71065729e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['배아 이식 경과일'] = train['배아 이식 경과일'].fillna(train['배아 이식 경과일'].mode()[0])\n",
    "test['배아 이식 경과일'] = test['배아 이식 경과일'].fillna(train['배아 이식 경과일'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820a0843-bbaf-4171-94bc-2401ac79b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 배아 이식 여부         6291\n",
      "착상 전 유전 진단 사용 여부    6291\n",
      "배아 생성 주요 이유         6291\n",
      "총 생성 배아 수           6291\n",
      "미세주입된 난자 수          6291\n",
      "미세주입에서 생성된 배아 수     6291\n",
      "이식된 배아 수            6291\n",
      "미세주입 배아 이식 수        6291\n",
      "저장된 배아 수            6291\n",
      "미세주입 후 저장된 배아 수     6291\n",
      "해동된 배아 수            6291\n",
      "해동 난자 수             6291\n",
      "수집된 신선 난자 수         6291\n",
      "저장된 신선 난자 수         6291\n",
      "혼합된 난자 수            6291\n",
      "파트너 정자와 혼합된 난자 수    6291\n",
      "기증자 정자와 혼합된 난자 수    6291\n",
      "동결 배아 사용 여부         6291\n",
      "신선 배아 사용 여부         6291\n",
      "기증 배아 사용 여부         6291\n",
      "대리모 여부              6291\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치가 존재하는 열과 개수 출력\n",
    "train_missing_values = train.isnull().sum()\n",
    "train_missing_columns = train_missing_values[train_missing_values > 0]  # 결측치가 있는 열만 필터링\n",
    "\n",
    "print(train_missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c31955-4b5f-4cfc-96ce-bc3eacfa241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 배아 이식 여부         2176\n",
      "착상 전 유전 진단 사용 여부    2176\n",
      "배아 생성 주요 이유         2176\n",
      "총 생성 배아 수           2176\n",
      "미세주입된 난자 수          2176\n",
      "미세주입에서 생성된 배아 수     2176\n",
      "이식된 배아 수            2176\n",
      "미세주입 배아 이식 수        2176\n",
      "저장된 배아 수            2176\n",
      "미세주입 후 저장된 배아 수     2176\n",
      "해동된 배아 수            2176\n",
      "해동 난자 수             2176\n",
      "수집된 신선 난자 수         2176\n",
      "저장된 신선 난자 수         2176\n",
      "혼합된 난자 수            2176\n",
      "파트너 정자와 혼합된 난자 수    2176\n",
      "기증자 정자와 혼합된 난자 수    2176\n",
      "동결 배아 사용 여부         2176\n",
      "신선 배아 사용 여부         2176\n",
      "기증 배아 사용 여부         2176\n",
      "대리모 여부              2176\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치가 존재하는 열과 개수 출력\n",
    "test_missing_values = test.isnull().sum()\n",
    "test_missing_columns = test_missing_values[test_missing_values > 0]  # 결측치가 있는 열만 필터링\n",
    "\n",
    "print(test_missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc84733b-7854-40b5-b1fb-7607fad4e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(train.mode().iloc[0], inplace=True)\n",
    "test.fillna(train.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01eba70e-091d-4c7a-9e2c-6f0b18036680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e429b804-4169-432f-bc85-90c6f5326a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 컬럼 '총 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '6회 이상' '3회' '4회' '5회']\n",
      "train 컬럼 '클리닉 내 총 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '6회 이상' '3회' '5회' '4회']\n",
      "train 컬럼 'IVF 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '5회' '4회' '6회 이상']\n",
      "train 컬럼 'DI 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '5회' '3회' '1회' '2회' '4회' '6회 이상']\n",
      "train 컬럼 '총 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회' '6회 이상']\n",
      "train 컬럼 'IVF 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회' '6회 이상']\n",
      "train 컬럼 'DI 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회']\n",
      "train 컬럼 '총 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '6회 이상' '5회']\n",
      "train 컬럼 'IVF 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회']\n",
      "train 컬럼 'DI 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '5회']\n",
      "train 컬럼 '난자 기증자 나이'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['알 수 없음' '만21-25세' '만31-35세' '만26-30세' '만20세 이하']\n",
      "train 컬럼 '정자 기증자 나이'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['알 수 없음' '만26-30세' '만21-25세' '만41-45세' '만36-40세' '만31-35세' '만20세 이하']\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼에서 숫자로 변환할 수 없는 값 확인\n",
    "for col in [ '총 시술 횟수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수',\n",
    "            'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '난자 기증자 나이', '정자 기증자 나이']:\n",
    "    # 변환할 수 없는 값 찾기\n",
    "    invalid_values = df[col][pd.to_numeric(df[col], errors='coerce').isna()]\n",
    "    if len(invalid_values) > 0:\n",
    "        print(f\"train 컬럼 '{col}'에 숫자로 변환할 수 없는 값이 있습니다:\")\n",
    "        print(invalid_values.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "642eae2f-f56d-4730-97c8-47d4d7405fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 컬럼 '총 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['1회' '0회' '3회' '5회' '2회' '4회' '6회 이상']\n",
      "test 컬럼 '클리닉 내 총 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['1회' '0회' '3회' '5회' '2회' '4회' '6회 이상']\n",
      "test 컬럼 'IVF 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['1회' '0회' '5회' '2회' '4회' '3회' '6회 이상']\n",
      "test 컬럼 'DI 시술 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '2회' '1회' '3회' '5회' '4회' '6회 이상']\n",
      "test 컬럼 '총 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회' '6회 이상']\n",
      "test 컬럼 'IVF 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회']\n",
      "test 컬럼 'DI 임신 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '4회' '5회']\n",
      "test 컬럼 '총 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '4회' '3회' '6회 이상']\n",
      "test 컬럼 'IVF 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '4회' '3회']\n",
      "test 컬럼 'DI 출산 횟수'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['0회' '1회' '2회' '3회' '5회']\n",
      "test 컬럼 '난자 기증자 나이'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['알 수 없음' '만26-30세' '만21-25세' '만31-35세' '만20세 이하']\n",
      "test 컬럼 '정자 기증자 나이'에 숫자로 변환할 수 없는 값이 있습니다:\n",
      "['알 수 없음' '만36-40세' '만31-35세' '만41-45세' '만26-30세' '만21-25세' '만20세 이하']\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼에서 숫자로 변환할 수 없는 값 확인\n",
    "for col in [ '총 시술 횟수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수',\n",
    "            'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수', '난자 기증자 나이', '정자 기증자 나이']:\n",
    "    # 변환할 수 없는 값 찾기\n",
    "    invalid_values = test[col][pd.to_numeric(test[col], errors='coerce').isna()]\n",
    "    if len(invalid_values) > 0:\n",
    "        print(f\"test 컬럼 '{col}'에 숫자로 변환할 수 없는 값이 있습니다:\")\n",
    "        print(invalid_values.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf94137-cfe5-4a34-9cc3-4a6c660b002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자로 변환할 수 없는 값들 처리\n",
    "def convert_to_numeric(value):\n",
    "    # '회'를 제거하고 숫자로 변환\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('회', '').replace('회 이상', '6')  # '6회 이상'을 6으로 변경\n",
    "        try:\n",
    "            return float(value)  # 숫자로 변환\n",
    "        except ValueError:\n",
    "            return None  # 변환할 수 없으면 None 반환\n",
    "    return value\n",
    "\n",
    "# 해당 컬럼들을 변환\n",
    "for col in ['총 시술 횟수', '클리닉 내 총 시술 횟수', 'IVF 시술 횟수', 'DI 시술 횟수', '총 임신 횟수',\n",
    "            'IVF 임신 횟수', 'DI 임신 횟수', '총 출산 횟수', 'IVF 출산 횟수', 'DI 출산 횟수']:\n",
    "    df[col] = df[col].apply(convert_to_numeric)\n",
    "    test[col]=test[col].apply(convert_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b689bc7-a296-4206-94d7-0dc6a63c88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '난자 기증자 나이'와 '정자 기증자 나이' 처리\n",
    "def age_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if value == '알 수 없음':\n",
    "            return None  # '알 수 없음'은 NaN으로 처리\n",
    "        age_groups = {\n",
    "            '만20세 이하': 20,\n",
    "            '만21-25세': 23,\n",
    "            '만26-30세': 28,\n",
    "            '만31-35세': 33,\n",
    "            '만36-40세': 38,\n",
    "            '만41-45세': 43\n",
    "        }\n",
    "        return age_groups.get(value, None)  # 해당 연령대의 중앙값을 반환\n",
    "    return value\n",
    "\n",
    "df['난자 기증자 나이'] = df['난자 기증자 나이'].apply(age_to_numeric)\n",
    "df['정자 기증자 나이'] = df['정자 기증자 나이'].apply(age_to_numeric)\n",
    "test['난자 기증자 나이'] = test['난자 기증자 나이'].apply(age_to_numeric)\n",
    "test['정자 기증자 나이'] = test['정자 기증자 나이'].apply(age_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "709dead4-3362-4bd9-b049-ba4e8d241c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID           0\n",
      "시술 시기 코드     0\n",
      "시술 당시 나이     0\n",
      "시술 유형        0\n",
      "특정 시술 유형     0\n",
      "            ..\n",
      "대리모 여부       0\n",
      "난자 채취 경과일    0\n",
      "난자 혼합 경과일    0\n",
      "배아 이식 경과일    0\n",
      "임신 성공 여부     0\n",
      "Length: 63, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# NaN 값이 있는지 다시 확인\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c08d5ad-cf0a-4805-94d2-528a4cbfb380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "시술 시기 코드       0\n",
      "시술 당시 나이       0\n",
      "시술 유형          0\n",
      "특정 시술 유형       0\n",
      "              ..\n",
      "기증 배아 사용 여부    0\n",
      "대리모 여부         0\n",
      "난자 채취 경과일      0\n",
      "난자 혼합 경과일      0\n",
      "배아 이식 경과일      0\n",
      "Length: 62, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# NaN 값이 있는지 다시 확인\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb809b25-1739-4c87-8671-eb469cad1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 ID 컬럼 제거\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "df.drop(columns=['시술 시기 코드'], inplace=True)\n",
    "test.drop(columns=['ID'], inplace=True)\n",
    "test.drop(columns=['시술 시기 코드'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c56de4fb-f5ac-4fc6-abec-175fbf9a5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_treatment(x):\n",
    "    if \"IVF\" in x:\n",
    "        return \"IVF\"\n",
    "    elif \"ICSI\" in x:\n",
    "        return \"ICSI\"\n",
    "    elif \"IUI\" in x or \"ICI\" in x:\n",
    "        return \"IUI\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# ✅ 그룹화 적용 (train & test)\n",
    "df[\"특정 시술 유형\"] = df[\"특정 시술 유형\"].apply(categorize_treatment)\n",
    "test[\"특정 시술 유형\"] = test[\"특정 시술 유형\"].apply(categorize_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17331fde-9330-42ed-83c7-0664de12162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n",
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\2842533252.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# 훈련 데이터에 대해 fit을 수행합니다.\n",
    "for col in ['시술 유형', '특정 시술 유형','배란 유도 유형', '배아 생성 주요 이유', '난자 출처', '정자 출처', '시술 당시 나이']:\n",
    "    # 훈련 데이터에서 fit하고 변환\n",
    "    df[col] = le.fit_transform(df[col].astype(str)) \n",
    "\n",
    "    # 테스트 데이터에서 훈련 데이터에 없는 레이블은 'unknown'으로 처리합니다.\n",
    "    test_col = test[col].astype(str)\n",
    "    test[col] = np.where(test_col.isin(le.classes_), le.transform(test_col), -1)  # -1을 'unknown'으로 대체\n",
    "\n",
    "# 결측치 처리\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "for col in test.columns:\n",
    "    if test[col].isnull().sum() > 0:\n",
    "        if test[col].dtype == 'object':\n",
    "            test[col].fillna(df[col].mode()[0], inplace=True)  # 훈련 데이터의 모드로 결측치 채우기\n",
    "        else:\n",
    "            test[col].fillna(df[col].median(), inplace=True)  # 훈련 데이터의 중앙값으로 결측치 채우기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0c713d-7f76-41d3-91d2-2114d2cc2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c671344b-a5be-48be-96ef-6e2d4a1f1382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 모델 학습 및 평가\\ndef evaluate_model(model, X_train, X_test, y_train, y_test):\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\\n    print(f\"{model.__class__.__name__}:\")\\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\\n    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\\n    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\\n\")\\n\\n# 모델 리스트\\nmodels = [RandomForestClassifier(), XGBClassifier(use_label_encoder=False, eval_metric=\\'logloss\\'), LGBMClassifier()]\\n\\n# 모델 성능 비교\\nfor model in models:\\n    evaluate_model(model, X_train, X_test, y_train, y_test)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 데이터 분할\n",
    "X = df.drop(columns=['임신 성공 여부'])\n",
    "y = df['임신 성공 여부']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\"\"\"\n",
    "# 모델 학습 및 평가\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(f\"{model.__class__.__name__}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\\n\")\n",
    "\n",
    "# 모델 리스트\n",
    "models = [RandomForestClassifier(), XGBClassifier(use_label_encoder=False, eval_metric='logloss'), LGBMClassifier()]\n",
    "\n",
    "# 모델 성능 비교\n",
    "for model in models:\n",
    "    evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7f6e0ef-f16a-46df-82e6-4bea2389fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom lightgbm import LGBMClassifier\\nimport numpy as np\\n\\n# LGBMClassifier 모델 정의\\nlgbm = LGBMClassifier()\\n\\n# 하이퍼파라미터 분포 설정 (랜덤 서치에서는 분포를 지정)\\nparam_dist = {\\n    \\'n_estimators\\': [100, 200, 300],\\n    \\'learning_rate\\': [0.01, 0.05, 0.1],\\n    \\'max_depth\\': [3, 5, 7],\\n    \\'num_leaves\\': [31, 50, 100],\\n    \\'subsample\\': [0.7, 0.8, 1.0],\\n    \\'colsample_bytree\\': [0.7, 0.8, 1.0]\\n}\\n\\n# RandomizedSearchCV 사용: 5-fold 교차 검증, 정확도 기반 평가\\nrandom_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=50, scoring=\\'accuracy\\', cv=5, n_jobs=-1)\\n\\n# 훈련 데이터로 최적 하이퍼파라미터 탐색\\nrandom_search.fit(X_train, y_train)\\n\\n# 최적 하이퍼파라미터 출력\\nprint(f\"Best parameters: {random_search.best_params_}\")\\n\\n# 최적 모델을 이용한 예측\\nbest_model = random_search.best_estimator_\\ny_pred = best_model.predict(X_test)\\n\\n# 성능 평가\\nfrom sklearn.metrics import accuracy_score\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "# LGBMClassifier 모델 정의\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# 하이퍼파라미터 분포 설정 (랜덤 서치에서는 분포를 지정)\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV 사용: 5-fold 교차 검증, 정확도 기반 평가\n",
    "random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=50, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "# 훈련 데이터로 최적 하이퍼파라미터 탐색\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# 최적 모델을 이용한 예측\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 성능 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d436e302-9e9b-42fb-8032-8aec59341cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom lightgbm import LGBMClassifier\\nfrom sklearn.metrics import roc_auc_score\\n\\n# LGBMClassifier 모델 정의\\nlgbm = LGBMClassifier()\\n\\n# 하이퍼파라미터 분포 설정 (랜덤 서치에서는 분포를 지정)\\nparam_dist = {\\n    \\'n_estimators\\': [100, 150, 200],  # 더 적은 값으로 축소\\n    \\'learning_rate\\': [0.01, 0.05],  # 적은 값으로 설정\\n    \\'max_depth\\': [3, 5],  # 범위 축소\\n    \\'num_leaves\\': [31, 50],  # 적은 값으로 축소\\n    \\'subsample\\': [0.7, 0.8],  # 범위 축소\\n    \\'colsample_bytree\\': [0.7, 0.8]  # 범위 축소\\n}\\n\\n# RandomizedSearchCV 사용: 3-fold 교차 검증, ROC AUC 점수 기반 평가\\nrandom_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=20, scoring=\\'roc_auc\\', cv=3, n_jobs=-1, verbose=1)\\n\\n# 훈련 데이터로 최적 하이퍼파라미터 탐색\\nrandom_search.fit(X_train, y_train)\\n\\n# 최적 하이퍼파라미터 출력\\nprint(f\"Best parameters: {random_search.best_params_}\")\\n\\n# 최적 모델을 이용한 예측\\nbest_model = random_search.best_estimator_\\ny_pred = best_model.predict(X_test)\\n\\n# ROC AUC 점수 출력\\nroc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\\nprint(f\"ROC AUC Score for the best model: {roc_auc:.4f}\")\\n\\n# 각 하이퍼파라미터 조합에 대한 ROC AUC 점수 출력\\nprint(\"\\nAll evaluated parameters and their ROC AUC scores:\")\\nfor mean_score, params in zip(random_search.cv_results_[\\'mean_test_score\\'], random_search.cv_results_[\\'params\\']):\\n    print(f\"ROC AUC: {mean_score:.4f} | Parameters: {params}\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# LGBMClassifier 모델 정의\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# 하이퍼파라미터 분포 설정 (랜덤 서치에서는 분포를 지정)\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 150, 200],  # 더 적은 값으로 축소\n",
    "    'learning_rate': [0.01, 0.05],  # 적은 값으로 설정\n",
    "    'max_depth': [3, 5],  # 범위 축소\n",
    "    'num_leaves': [31, 50],  # 적은 값으로 축소\n",
    "    'subsample': [0.7, 0.8],  # 범위 축소\n",
    "    'colsample_bytree': [0.7, 0.8]  # 범위 축소\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV 사용: 3-fold 교차 검증, ROC AUC 점수 기반 평가\n",
    "random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=20, scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# 훈련 데이터로 최적 하이퍼파라미터 탐색\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "# 최적 모델을 이용한 예측\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# ROC AUC 점수 출력\n",
    "roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "print(f\"ROC AUC Score for the best model: {roc_auc:.4f}\")\n",
    "\n",
    "# 각 하이퍼파라미터 조합에 대한 ROC AUC 점수 출력\n",
    "print(\"\\nAll evaluated parameters and their ROC AUC scores:\")\n",
    "for mean_score, params in zip(random_search.cv_results_['mean_test_score'], random_search.cv_results_['params']):\n",
    "    print(f\"ROC AUC: {mean_score:.4f} | Parameters: {params}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9090940-624d-47d2-924a-fd61c743ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 53102, number of negative: 151978\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "Accuracy: 0.7478\n",
      "F1 Score: 0.1792\n",
      "ROC AUC Score: 0.7376\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 모델 정의 및 학습\n",
    "model = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # ROC AUC 평가를 위한 확률값\n",
    "\n",
    "# 성능 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de45423d-9798-45c9-8ee0-a8934fdf0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc7a2311-d5af-443f-9595-e6867264059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_proba(test)[:, 1]  # 클래스 1(임신 성공) 확률만 선택\n",
    "a['probability'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "091f2ceb-e7b2-4f45-9e91-5afc0411eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.to_csv('prediction3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35c165b9-2280-4887-ab1b-d7947c69afb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 53102, number of negative: 151978\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051521\n",
      "[LightGBM] [Info] Start training from score -1.051521\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "ROC AUC Score: 0.7379\n",
      "Accuracy: 0.7476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85     38145\n",
      "           1       0.54      0.11      0.18     13126\n",
      "\n",
      "    accuracy                           0.75     51271\n",
      "   macro avg       0.65      0.54      0.51     51271\n",
      "weighted avg       0.70      0.75      0.68     51271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 주어진 하이퍼파라미터로 LGBMClassifier 모델 정의\n",
    "lgbm_model = LGBMClassifier(\n",
    "    subsample=0.7,\n",
    "    num_leaves=31,\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "y_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]  # ROC AUC를 위해 확률값 추출\n",
    "\n",
    "# ROC AUC 점수 출력\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# 추가적인 성능 평가 (예: accuracy, precision, recall, f1-score 등)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d736c18d-70e8-4ea9-b87e-8947a9b2bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = lgbm_model.predict_proba(test)[:, 1] \n",
    "a['probability'] = pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b48ca4cc-4c6d-40dc-8b83-e81937eb87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.to_csv('prediction2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8231d6c-ccd0-4920-8d5a-e854b3d67247",
   "metadata": {},
   "source": [
    "### 추가 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d44cc67-abd5-48a5-9fbb-0369e9e829d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256351 entries, 0 to 256350\n",
      "Data columns (total 60 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   시술 당시 나이            256351 non-null  int32  \n",
      " 1   시술 유형               256351 non-null  int32  \n",
      " 2   특정 시술 유형            256351 non-null  int32  \n",
      " 3   배란 자극 여부            256351 non-null  int64  \n",
      " 4   배란 유도 유형            256351 non-null  int32  \n",
      " 5   단일 배아 이식 여부         256351 non-null  float64\n",
      " 6   착상 전 유전 진단 사용 여부    256351 non-null  float64\n",
      " 7   남성 주 불임 원인          256351 non-null  int64  \n",
      " 8   남성 부 불임 원인          256351 non-null  int64  \n",
      " 9   여성 주 불임 원인          256351 non-null  int64  \n",
      " 10  여성 부 불임 원인          256351 non-null  int64  \n",
      " 11  부부 주 불임 원인          256351 non-null  int64  \n",
      " 12  부부 부 불임 원인          256351 non-null  int64  \n",
      " 13  불명확 불임 원인           256351 non-null  int64  \n",
      " 14  불임 원인 - 난관 질환       256351 non-null  int64  \n",
      " 15  불임 원인 - 남성 요인       256351 non-null  int64  \n",
      " 16  불임 원인 - 배란 장애       256351 non-null  int64  \n",
      " 17  불임 원인 - 여성 요인       256351 non-null  int64  \n",
      " 18  불임 원인 - 자궁경부 문제     256351 non-null  int64  \n",
      " 19  불임 원인 - 자궁내막증       256351 non-null  int64  \n",
      " 20  불임 원인 - 정자 농도       256351 non-null  int64  \n",
      " 21  불임 원인 - 정자 면역학적 요인  256351 non-null  int64  \n",
      " 22  불임 원인 - 정자 운동성      256351 non-null  int64  \n",
      " 23  불임 원인 - 정자 형태       256351 non-null  int64  \n",
      " 24  배아 생성 주요 이유         256351 non-null  int32  \n",
      " 25  총 시술 횟수             256351 non-null  float64\n",
      " 26  클리닉 내 총 시술 횟수       256351 non-null  float64\n",
      " 27  IVF 시술 횟수           256351 non-null  float64\n",
      " 28  DI 시술 횟수            256351 non-null  float64\n",
      " 29  총 임신 횟수             256351 non-null  float64\n",
      " 30  IVF 임신 횟수           256351 non-null  float64\n",
      " 31  DI 임신 횟수            256351 non-null  float64\n",
      " 32  총 출산 횟수             256351 non-null  float64\n",
      " 33  IVF 출산 횟수           256351 non-null  float64\n",
      " 34  DI 출산 횟수            256351 non-null  float64\n",
      " 35  총 생성 배아 수           256351 non-null  float64\n",
      " 36  미세주입된 난자 수          256351 non-null  float64\n",
      " 37  미세주입에서 생성된 배아 수     256351 non-null  float64\n",
      " 38  이식된 배아 수            256351 non-null  float64\n",
      " 39  미세주입 배아 이식 수        256351 non-null  float64\n",
      " 40  저장된 배아 수            256351 non-null  float64\n",
      " 41  미세주입 후 저장된 배아 수     256351 non-null  float64\n",
      " 42  해동된 배아 수            256351 non-null  float64\n",
      " 43  해동 난자 수             256351 non-null  float64\n",
      " 44  수집된 신선 난자 수         256351 non-null  float64\n",
      " 45  저장된 신선 난자 수         256351 non-null  float64\n",
      " 46  혼합된 난자 수            256351 non-null  float64\n",
      " 47  파트너 정자와 혼합된 난자 수    256351 non-null  float64\n",
      " 48  기증자 정자와 혼합된 난자 수    256351 non-null  float64\n",
      " 49  난자 출처               256351 non-null  int32  \n",
      " 50  정자 출처               256351 non-null  int32  \n",
      " 51  난자 기증자 나이           256351 non-null  float64\n",
      " 52  정자 기증자 나이           256351 non-null  float64\n",
      " 53  동결 배아 사용 여부         256351 non-null  float64\n",
      " 54  신선 배아 사용 여부         256351 non-null  float64\n",
      " 55  기증 배아 사용 여부         256351 non-null  float64\n",
      " 56  대리모 여부              256351 non-null  float64\n",
      " 57  난자 채취 경과일           256351 non-null  float64\n",
      " 58  난자 혼합 경과일           256351 non-null  float64\n",
      " 59  배아 이식 경과일           256351 non-null  float64\n",
      "dtypes: float64(35), int32(7), int64(18)\n",
      "memory usage: 110.5 MB\n",
      "None\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 256351 entries, 0 to 256350\n",
      "Series name: 임신 성공 여부\n",
      "Non-Null Count   Dtype\n",
      "--------------   -----\n",
      "256351 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.info())\n",
    "print(y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa49df0c-7ab6-4bee-b4d2-18cc3112df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164064, 60)\n",
      "(51271, 60)\n",
      "(164064,)\n",
      "(51271,)\n",
      "(41016, 60)\n",
      "(41016,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X,y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=42, stratify=Y_train\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8071f6be-2a52-4666-965a-b3d66c126554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    'Extra Trees': ExtraTreesClassifier(max_depth= None, min_samples_split = 2, n_estimators = 200, random_state=42, n_jobs=-1),\n",
    "    'CatBoost': CatBoostClassifier(depth = 10, iterations= 200, learning_rate= 0.1, verbose=0, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(max_depth=10, learning_rate= 0.1, n_estimators= 300, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(max_depth = 10, learning_rate= 0.1, n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    'Random Forest': RandomForestClassifier(max_depth=10, n_estimators=200, class_weight=\"balanced\", random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f413fc2-8db1-4bf4-8561-6bf575812d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3ef0fa-4de3-48bb-8ac4-5270db1b1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE + Under-sampling: {0: 171110, 1: 171110}\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 1️⃣ SMOTE 적용 (1 클래스 샘플 증가)\n",
    "smote = SMOTE(sampling_strategy=0.9, random_state=42)  # 1 클래스가 0 클래스의 80%가 되도록 증강\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 2️⃣ 언더샘플링 적용 (0 클래스 샘플 감소)\n",
    "under_sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)  # 자동 조정\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"After SMOTE + Under-sampling:\", pd.Series(y_resampled).value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a1d662b-df9d-4610-9987-cb3c0f2247c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (256351, 60)\n",
      "Y_train shape: (256351,)\n",
      "X_resampled shape: (342220, 60)\n",
      "y_resampled shape: (342220,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X.shape)\n",
    "print(\"Y_train shape:\", y.shape)\n",
    "\n",
    "print(\"X_resampled shape:\", X_resampled.shape)\n",
    "print(\"y_resampled shape:\", y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8dd3e60-2bbb-4fa9-a65d-0890c2f70f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c7f261c-ac93-4160-ba29-10057dad06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ✅ 각 모델별 Test 예측값을 저장할 리스트 (Soft Voting 적용을 위한 확률 평균)\\ntest_predictions = {name: np.zeros((X_test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\\nsubmission_predictions = {name: np.zeros((test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\\n\\n# ✅ KFold 적용\\nroc_auc_scores = []  # ROC-AUC 점수 저장 리스트\\n\\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled)):\\n    print(f\"\\n=== Fold {fold+1} ===\")\\n\\n    # KFold 데이터 분할\\n    X_train_fold, X_val_fold = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\\n    Y_train_fold, Y_val_fold = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\\n\\n    for name, model in models.items():\\n        if hasattr(model, \"predict_proba\"):  # `predict_proba()`를 지원하는 모델만 사용\\n            print(f\"\\n--- Training {name} ---\")\\n            \\n            # 모델 학습\\n            model.fit(X_train_fold, Y_train_fold)\\n\\n            # 훈련 세트 성능 평가\\n            Y_train_pred = model.predict(X_train_fold)\\n            print(\"\\n--- Training Set Performance ---\")\\n            print(classification_report(Y_train_fold, Y_train_pred))\\n\\n            # 검증 세트 성능 평가\\n            Y_val_pred = model.predict(X_val_fold)\\n            print(\"\\n--- Validation Set Performance ---\")\\n            print(classification_report(Y_val_fold, Y_val_pred))\\n\\n            # ✅ ROC-AUC 점수 계산 및 저장\\n            Y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]  # Positive Class 확률\\n            roc_auc = roc_auc_score(Y_val_fold, Y_val_pred_proba)\\n            roc_auc_scores.append(roc_auc)\\n            print(f\"📊 ROC-AUC (Fold {fold+1}, {name}): {roc_auc:.4f}\")\\n\\n            # ✅ X_test 예측값 저장 (Soft Voting을 위해 확률 예측값 사용)\\n            test_predictions[name] += model.predict_proba(X_test) / kf.n_splits  # 확률 평균 계산\\n            \\n            # ✅ 실제 test.csv 데이터 예측값 저장\\n            submission_predictions[name] += model.predict_proba(test) / kf.n_splits  # 확률 평균 계산\\n\\n        else:\\n            print(f\"⚠ Warning: {name} 모델은 `predict_proba()`를 지원하지 않습니다. 제외됨.\")\\n\\n# ✅ Soft Voting을 위한 최종 예측값 생성 (개별 모델 결과 확률 평균)\\nensemble_pred_proba = np.mean(list(test_predictions.values()), axis=0)\\nsubmission_pred_proba = np.mean(list(submission_predictions.values()), axis=0)  # 실제 test.csv 예측값\\n\\n# ✅ 확률 기반 예측값 변환 (Soft Voting)\\nY_test_pred_soft_voting = np.argmax(ensemble_pred_proba, axis=1)\\n\\nprint(\"\\n=== Final Test Evaluation (Soft Voting) ===\")\\nprint(\"\\n--- Test Set Performance ---\")\\nprint(classification_report(Y_test, Y_test_pred_soft_voting))\\n\\n# ✅ Soft Voting을 위한 확률 예측값 저장 (Positive Class 확률)\\nfinal_probabilities = ensemble_pred_proba[:, 1]  # 검증 데이터 (X_test) Positive Class 확률\\nsubmission_probabilities = submission_pred_proba[:, 1]  # 제출 데이터 (test.csv) Positive Class 확률\\n\\n# ✅ 전체 ROC-AUC 점수 출력\\naverage_roc_auc = np.mean(roc_auc_scores)\\nprint(f\"\\n📈 평균 ROC-AUC 점수 (KFold 전체): {average_roc_auc:.4f}\")\\n\\n# ✅ ROC-AUC 점수를 파일명에 포함하여 저장\\nsubmission_filename = f\\'./data/submission_{average_roc_auc:.4f}.csv\\'\\n\\n# ✅ Kaggle 제출 파일 생성\\n# sample_submission = pd.read_csv(\\'./data/sample_submission.csv\\')  # 제출 템플릿 불러오기\\n# sample_submission[\\'probability\\'] = submission_probabilities  # 확률 값 저장\\n\\n# ✅ CSV 저장 (index=False)\\n# sample_submission.to_csv(submission_filename, index=False)\\n\\n# print(f\"\\n✅ 제출 파일이 생성되었습니다: {submission_filename}\")\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ✅ 각 모델별 Test 예측값을 저장할 리스트 (Soft Voting 적용을 위한 확률 평균)\n",
    "test_predictions = {name: np.zeros((X_test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\n",
    "submission_predictions = {name: np.zeros((test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\n",
    "\n",
    "# ✅ KFold 적용\n",
    "roc_auc_scores = []  # ROC-AUC 점수 저장 리스트\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    # KFold 데이터 분할\n",
    "    X_train_fold, X_val_fold = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n",
    "    Y_train_fold, Y_val_fold = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, \"predict_proba\"):  # `predict_proba()`를 지원하는 모델만 사용\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            # 모델 학습\n",
    "            model.fit(X_train_fold, Y_train_fold)\n",
    "\n",
    "            # 훈련 세트 성능 평가\n",
    "            Y_train_pred = model.predict(X_train_fold)\n",
    "            print(\"\\n--- Training Set Performance ---\")\n",
    "            print(classification_report(Y_train_fold, Y_train_pred))\n",
    "\n",
    "            # 검증 세트 성능 평가\n",
    "            Y_val_pred = model.predict(X_val_fold)\n",
    "            print(\"\\n--- Validation Set Performance ---\")\n",
    "            print(classification_report(Y_val_fold, Y_val_pred))\n",
    "\n",
    "            # ✅ ROC-AUC 점수 계산 및 저장\n",
    "            Y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]  # Positive Class 확률\n",
    "            roc_auc = roc_auc_score(Y_val_fold, Y_val_pred_proba)\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "            print(f\"📊 ROC-AUC (Fold {fold+1}, {name}): {roc_auc:.4f}\")\n",
    "\n",
    "            # ✅ X_test 예측값 저장 (Soft Voting을 위해 확률 예측값 사용)\n",
    "            test_predictions[name] += model.predict_proba(X_test) / kf.n_splits  # 확률 평균 계산\n",
    "            \n",
    "            # ✅ 실제 test.csv 데이터 예측값 저장\n",
    "            submission_predictions[name] += model.predict_proba(test) / kf.n_splits  # 확률 평균 계산\n",
    "\n",
    "        else:\n",
    "            print(f\"⚠ Warning: {name} 모델은 `predict_proba()`를 지원하지 않습니다. 제외됨.\")\n",
    "\n",
    "# ✅ Soft Voting을 위한 최종 예측값 생성 (개별 모델 결과 확률 평균)\n",
    "ensemble_pred_proba = np.mean(list(test_predictions.values()), axis=0)\n",
    "submission_pred_proba = np.mean(list(submission_predictions.values()), axis=0)  # 실제 test.csv 예측값\n",
    "\n",
    "# ✅ 확률 기반 예측값 변환 (Soft Voting)\n",
    "Y_test_pred_soft_voting = np.argmax(ensemble_pred_proba, axis=1)\n",
    "\n",
    "print(\"\\n=== Final Test Evaluation (Soft Voting) ===\")\n",
    "print(\"\\n--- Test Set Performance ---\")\n",
    "print(classification_report(Y_test, Y_test_pred_soft_voting))\n",
    "\n",
    "# ✅ Soft Voting을 위한 확률 예측값 저장 (Positive Class 확률)\n",
    "final_probabilities = ensemble_pred_proba[:, 1]  # 검증 데이터 (X_test) Positive Class 확률\n",
    "submission_probabilities = submission_pred_proba[:, 1]  # 제출 데이터 (test.csv) Positive Class 확률\n",
    "\n",
    "# ✅ 전체 ROC-AUC 점수 출력\n",
    "average_roc_auc = np.mean(roc_auc_scores)\n",
    "print(f\"\\n📈 평균 ROC-AUC 점수 (KFold 전체): {average_roc_auc:.4f}\")\n",
    "\n",
    "# ✅ ROC-AUC 점수를 파일명에 포함하여 저장\n",
    "submission_filename = f'./data/submission_{average_roc_auc:.4f}.csv'\n",
    "\n",
    "# ✅ Kaggle 제출 파일 생성\n",
    "# sample_submission = pd.read_csv('./data/sample_submission.csv')  # 제출 템플릿 불러오기\n",
    "# sample_submission['probability'] = submission_probabilities  # 확률 값 저장\n",
    "\n",
    "# ✅ CSV 저장 (index=False)\n",
    "# sample_submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "# print(f\"\\n✅ 제출 파일이 생성되었습니다: {submission_filename}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9485541-dd52-4b2c-ac63-6dfbd3c495ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission_probabilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msubmission_probabilities\u001b[49m \n",
      "\u001b[1;31mNameError\u001b[0m: name 'submission_probabilities' is not defined"
     ]
    }
   ],
   "source": [
    "a['probability'] = submission_probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eac55e-d10d-4210-a672-c7e50976a6b4",
   "metadata": {},
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99425e9b-73ec-4c19-8b1a-5d5bd319c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    " a.to_csv('prediction4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be7d38-eda7-4ddb-97ac-51e9afcc7d67",
   "metadata": {},
   "source": [
    "### 추가 실험2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e54d0e6-f88b-470e-9ce4-1782b2bed22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91c57ade-9206-4703-b505-54876d19fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7841491-005c-4216-8b1e-67d898012530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ✅ 각 모델별 Test 예측값을 저장할 리스트 (Soft Voting 적용을 위한 확률 평균)\n",
    "test_predictions = {name: np.zeros((X_test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\n",
    "submission_predictions = {name: np.zeros((test.shape[0], 2)) for name in models.keys() if hasattr(models[name], \"predict_proba\")}\n",
    "\n",
    "# ✅ KFold 적용\n",
    "roc_auc_scores = []  # ROC-AUC 점수 저장 리스트\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    # KFold 데이터 분할\n",
    "    X_train_fold, X_val_fold = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n",
    "    Y_train_fold, Y_val_fold = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, \"predict_proba\"):  # `predict_proba()`를 지원하는 모델만 사용\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            # 모델 학습\n",
    "            model.fit(X_train_fold, Y_train_fold)\n",
    "\n",
    "            # 훈련 세트 성능 평가\n",
    "            Y_train_pred = model.predict(X_train_fold)\n",
    "            print(\"\\n--- Training Set Performance ---\")\n",
    "            print(classification_report(Y_train_fold, Y_train_pred))\n",
    "\n",
    "            # 검증 세트 성능 평가\n",
    "            Y_val_pred = model.predict(X_val_fold)\n",
    "            print(\"\\n--- Validation Set Performance ---\")\n",
    "            print(classification_report(Y_val_fold, Y_val_pred))\n",
    "\n",
    "            # ✅ ROC-AUC 점수 계산 및 저장\n",
    "            Y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]  # Positive Class 확률\n",
    "            roc_auc = roc_auc_score(Y_val_fold, Y_val_pred_proba)\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "            print(f\"📊 ROC-AUC (Fold {fold+1}, {name}): {roc_auc:.4f}\")\n",
    "\n",
    "            # ✅ X_test 예측값 저장 (Soft Voting을 위해 확률 예측값 사용)\n",
    "            test_predictions[name] += model.predict_proba(X_test) / kf.n_splits  # 확률 평균 계산\n",
    "            \n",
    "            # ✅ 실제 test.csv 데이터 예측값 저장\n",
    "            submission_predictions[name] += model.predict_proba(test) / kf.n_splits  # 확률 평균 계산\n",
    "\n",
    "        else:\n",
    "            print(f\"⚠ Warning: {name} 모델은 `predict_proba()`를 지원하지 않습니다. 제외됨.\")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ✅ Platt Scaling (Sigmoid) 적용\n",
    "calibrated_lgbm = CalibratedClassifierCV(models[\"LightGBM\"], method=\"sigmoid\", cv=5)\n",
    "calibrated_lgbm.fit(X_resampled, y_resampled)\n",
    "\n",
    "# ✅ 보정된 확률 예측\n",
    "calibrated_pred_proba_test = calibrated_lgbm.predict_proba(X_test)[:, 1]\n",
    "calibrated_pred_proba_submission = calibrated_lgbm.predict_proba(test)[:, 1]\n",
    "\n",
    "# ✅ ROC-AUC 평가\n",
    "calibrated_roc_auc = roc_auc_score(Y_test, calibrated_pred_proba_test)\n",
    "print(f\"\\n📊 Calibrated LightGBM ROC-AUC Score (Sigmoid): {calibrated_roc_auc:.4f}\")\n",
    "\n",
    "# ✅ test.csv에 대한 예측 확률 보정 적용\n",
    "calibrated_pred_proba_submission = calibrated_lgbm.predict_proba(test)[:, 1]\n",
    "\n",
    "# ✅ Soft Voting 가중치 변경 (LightGBM의 영향력 증가)\n",
    "ensemble_pred_proba_test = (\n",
    "    calibrated_pred_proba_test * 0.6 +  # LightGBM 60%\n",
    "    test_predictions[\"XGBoost\"][:, 1] * 0.2 +  # XGBoost 20%\n",
    "    test_predictions[\"CatBoost\"][:, 1] * 0.2  # CatBoost 20%\n",
    ")\n",
    "\n",
    "ensemble_pred_proba_submission = (\n",
    "    calibrated_pred_proba_submission * 0.6 +\n",
    "    submission_predictions[\"XGBoost\"][:, 1] * 0.2 +\n",
    "    submission_predictions[\"CatBoost\"][:, 1] * 0.2\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 최적 Threshold 찾기\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ✅ Precision-Recall Curve 기반 최적 Threshold 찾기\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ensemble_pred_proba_test)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[f1_scores[:-1].argmax()]\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# ✅ 최적 Threshold 적용 후 예측값 변환\n",
    "Y_test_pred = (calibrated_pred_proba_test > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Final Test Evaluation with Optimized Threshold ===\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "# ✅ Kaggle 제출 파일 생성\n",
    "submission_filename = f'./data/submission_calibrated_ensemble_{calibrated_roc_auc:.4f}.csv'\n",
    "\n",
    "# ✅ Kaggle 제출 파일 저장\n",
    "\n",
    "# sample_submission['probability'] = ensemble_pred_proba_submission\n",
    "# sample_submission.to_csv(submission_filename, index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b94834-4531-4d95-9d4f-6ab7a997124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['probability'] = ensemble_pred_proba_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71346a5-7b6c-40e9-9a9e-1b651dc78416",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5d338-2dfc-4871-9a94-1d6c5c37ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    " a.to_csv('prediction5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa616a-adb8-486f-89ae-4b49c5a3108d",
   "metadata": {},
   "source": [
    "### 추가 실험3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f1c2f-c8c2-4cff-a8ed-5811a0654703",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eea71756-52a9-40e6-a033-73e625c8e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.258933\n",
      "ROC-AUC Score: 0.7371\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM 회귀 모델 (확률 예측)\n",
    "model = LGBMRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측값 (0~1 사이의 확률)\n",
    "y_pred_proba = model.predict(X_test)\n",
    "\n",
    "# 확률 값을 0~1 사이로 보정 (sigmoid 적용 가능)\n",
    "y_pred_proba = 1 / (1 + np.exp(-y_pred_proba))  # Sigmoid 적용 (선택 사항)\n",
    "\n",
    "# ROC-AUC 평가\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95725ecc-906d-49a3-b521-f1986fc9fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (2.1.3)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 6.6/124.9 MB 31.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 13.9/124.9 MB 32.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 21.5/124.9 MB 33.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 26.5/124.9 MB 31.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 33.3/124.9 MB 31.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 41.2/124.9 MB 32.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 48.0/124.9 MB 32.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 55.8/124.9 MB 32.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 64.5/124.9 MB 33.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 69.5/124.9 MB 33.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 76.5/124.9 MB 32.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 84.1/124.9 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 87.3/124.9 MB 31.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 90.4/124.9 MB 30.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 96.2/124.9 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 100.7/124.9 MB 29.5 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 101.4/124.9 MB 28.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 104.3/124.9 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 108.5/124.9 MB 26.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 114.8/124.9 MB 26.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.6/124.9 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 27.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 25.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.1.3\n",
      "    Uninstalling xgboost-2.1.3:\n",
      "      Successfully uninstalled xgboost-2.1.3\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\jeongbin\\Projects\\lg_aimers_project\\venv_310\\Lib\\site-packages\\~gboost'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\jeongbin\\projects\\lg_aimers_project\\venv_310\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1e5d73b-ffd1-4032-b7d4-acb1cf3236d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Optimizing CatBoost...\n",
      "✅ Best Params for CatBoost: {'learning_rate': 0.1, 'iterations': 100, 'depth': 6}\n",
      "\n",
      "🔍 Optimizing LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.258933\n",
      "✅ Best Params for LightGBM: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.05}\n",
      "\n",
      "🔍 Optimizing Extra Trees...\n",
      "✅ Best Params for Extra Trees: {'n_estimators': 100, 'max_depth': 10}\n",
      "\n",
      "🔍 Optimizing Random Forest...\n",
      "✅ Best Params for Random Forest: {'n_estimators': 300, 'max_depth': 10}\n",
      "\n",
      "🚀 Training Best CatBoost Model...\n",
      "📈 CatBoost ROC-AUC Score: 0.7367\n",
      "\n",
      "🚀 Training Best LightGBM Model...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 645\n",
      "[LightGBM] [Info] Number of data points in the train set: 205080, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.258933\n",
      "📈 LightGBM ROC-AUC Score: 0.7371\n",
      "\n",
      "🚀 Training Best Extra Trees Model...\n",
      "📈 Extra Trees ROC-AUC Score: 0.7336\n",
      "\n",
      "🚀 Training Best Random Forest Model...\n",
      "📈 Random Forest ROC-AUC Score: 0.7341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 모델 정의\n",
    "models = {\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(random_state=42, n_jobs=-1),\n",
    "    \"Extra Trees\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "# ✅ 간소화된 하이퍼파라미터 탐색 공간\n",
    "param_spaces = {\n",
    "    \"CatBoost\": {\"depth\": [6, 8], \"iterations\": [100, 300], \"learning_rate\": [0.05, 0.1]},\n",
    "    \"LightGBM\": {\"max_depth\": [7, 10], \"learning_rate\": [0.05, 0.1], \"n_estimators\": [100, 300]},\n",
    "    \"Extra Trees\": {\"max_depth\": [7, 10], \"n_estimators\": [100, 300]},\n",
    "    \"Random Forest\": {\"max_depth\": [7, 10], \"n_estimators\": [100, 300]},\n",
    "}\n",
    "\n",
    "# ✅ 최적 하이퍼파라미터 저장\n",
    "best_params = {}\n",
    "\n",
    "# ✅ 모델별 하이퍼파라미터 최적화\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔍 Optimizing {name}...\")\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_spaces[name],\n",
    "        n_iter=4,  # 🔥 탐색 횟수 줄임\n",
    "        cv=2,  # 🔥 교차 검증 2-Fold로 줄임\n",
    "        scoring=\"neg_mean_squared_error\",  # 회귀에서 MSE로 평가\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    best_params[name] = search.best_params_\n",
    "    print(f\"✅ Best Params for {name}: {search.best_params_}\")\n",
    "\n",
    "# ✅ 최적 모델로 예측 및 평가\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🚀 Training Best {name} Model...\")\n",
    "    model.set_params(**best_params[name])\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)  # 예측 값\n",
    "    \n",
    "    # Sigmoid 변환 (회귀 모델의 예측값을 확률로 변환)\n",
    "    y_pred_prob = 1 / (1 + np.exp(-y_pred))  # Sigmoid 적용\n",
    "    \n",
    "    # ROC-AUC 계산\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(f\"📈 {name} ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57dae8c-cc37-4a70-81d9-80b4f1c0090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"XGBoost\": XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": {\"max_depth\": [7, 10], \"learning_rate\": [0.05, 0.1], \"n_estimators\": [100, 300]},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f240415-dc7b-4d4d-8308-928dbe8f718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = {\n",
    "    \"Extra Trees\": ExtraTreesRegressor(max_depth= 10, min_samples_split = 2, n_estimators = 100, random_state=42, n_jobs=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(depth = 6, iterations= 100, learning_rate= 0.1, verbose=0, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(max_depth=7, learning_rate= 0.05, n_estimators= 100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBRegressor(max_depth = 10, learning_rate= 0.1, n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestRegressor(max_depth=10, n_estimators=300, random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8caab846-1035-441c-981b-5c6cdeafde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60f2e4b1-e978-4e95-9640-83adb4f29275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "\n",
      "--- Training Extra Trees ---\n",
      "📊 ROC-AUC (Fold 1, Extra Trees): 0.8011\n",
      "\n",
      "--- Training CatBoost ---\n",
      "📊 ROC-AUC (Fold 1, CatBoost): 0.8491\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7608\n",
      "[LightGBM] [Info] Number of data points in the train set: 273776, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.500478\n",
      "📊 ROC-AUC (Fold 1, LightGBM): 0.8528\n",
      "\n",
      "--- Training XGBoost ---\n",
      "📊 ROC-AUC (Fold 1, XGBoost): 0.8756\n",
      "\n",
      "--- Training Random Forest ---\n",
      "📊 ROC-AUC (Fold 1, Random Forest): 0.8125\n",
      "\n",
      "=== Fold 2 ===\n",
      "\n",
      "--- Training Extra Trees ---\n",
      "📊 ROC-AUC (Fold 2, Extra Trees): 0.8033\n",
      "\n",
      "--- Training CatBoost ---\n",
      "📊 ROC-AUC (Fold 2, CatBoost): 0.8511\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7609\n",
      "[LightGBM] [Info] Number of data points in the train set: 273776, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.499719\n",
      "📊 ROC-AUC (Fold 2, LightGBM): 0.8537\n",
      "\n",
      "--- Training XGBoost ---\n",
      "📊 ROC-AUC (Fold 2, XGBoost): 0.8784\n",
      "\n",
      "--- Training Random Forest ---\n",
      "📊 ROC-AUC (Fold 2, Random Forest): 0.8166\n",
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "--- Training Extra Trees ---\n",
      "📊 ROC-AUC (Fold 3, Extra Trees): 0.8046\n",
      "\n",
      "--- Training CatBoost ---\n",
      "📊 ROC-AUC (Fold 3, CatBoost): 0.8508\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7606\n",
      "[LightGBM] [Info] Number of data points in the train set: 273776, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.499569\n",
      "📊 ROC-AUC (Fold 3, LightGBM): 0.8555\n",
      "\n",
      "--- Training XGBoost ---\n",
      "📊 ROC-AUC (Fold 3, XGBoost): 0.8776\n",
      "\n",
      "--- Training Random Forest ---\n",
      "📊 ROC-AUC (Fold 3, Random Forest): 0.8158\n",
      "\n",
      "=== Fold 4 ===\n",
      "\n",
      "--- Training Extra Trees ---\n",
      "📊 ROC-AUC (Fold 4, Extra Trees): 0.8028\n",
      "\n",
      "--- Training CatBoost ---\n",
      "📊 ROC-AUC (Fold 4, CatBoost): 0.8500\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7444\n",
      "[LightGBM] [Info] Number of data points in the train set: 273776, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.499529\n",
      "📊 ROC-AUC (Fold 4, LightGBM): 0.8540\n",
      "\n",
      "--- Training XGBoost ---\n",
      "📊 ROC-AUC (Fold 4, XGBoost): 0.8762\n",
      "\n",
      "--- Training Random Forest ---\n",
      "📊 ROC-AUC (Fold 4, Random Forest): 0.8140\n",
      "\n",
      "=== Fold 5 ===\n",
      "\n",
      "--- Training Extra Trees ---\n",
      "📊 ROC-AUC (Fold 5, Extra Trees): 0.8045\n",
      "\n",
      "--- Training CatBoost ---\n",
      "📊 ROC-AUC (Fold 5, CatBoost): 0.8502\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7611\n",
      "[LightGBM] [Info] Number of data points in the train set: 273776, number of used features: 56\n",
      "[LightGBM] [Info] Start training from score 0.500705\n",
      "📊 ROC-AUC (Fold 5, LightGBM): 0.8535\n",
      "\n",
      "--- Training XGBoost ---\n",
      "📊 ROC-AUC (Fold 5, XGBoost): 0.8765\n",
      "\n",
      "--- Training Random Forest ---\n",
      "📊 ROC-AUC (Fold 5, Random Forest): 0.8143\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of CalibratedClassifierCV must be an object implementing 'fit' and 'predict_proba', an object implementing 'fit' and 'decision_function' or None. Got LGBMRegressor(learning_rate=0.05, max_depth=7, n_jobs=-1, random_state=42) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# ✅ LightGBM Platt Scaling (Sigmoid) 적용\u001b[39;00m\n\u001b[0;32m     45\u001b[0m calibrated_lgbm \u001b[38;5;241m=\u001b[39m CalibratedClassifierCV(models2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m\"\u001b[39m], method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mcalibrated_lgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# ✅ 보정된 확률 예측\u001b[39;00m\n\u001b[0;32m     49\u001b[0m calibrated_pred_proba_test \u001b[38;5;241m=\u001b[39m calibrated_lgbm\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1382\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of CalibratedClassifierCV must be an object implementing 'fit' and 'predict_proba', an object implementing 'fit' and 'decision_function' or None. Got LGBMRegressor(learning_rate=0.05, max_depth=7, n_jobs=-1, random_state=42) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 모델 정의\n",
    "models2 = {\n",
    "    \"Extra Trees\": ExtraTreesRegressor(max_depth=10, min_samples_split=2, n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(depth=6, iterations=100, learning_rate=0.1, verbose=0, random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(max_depth=7, learning_rate=0.05, n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(max_depth=10, learning_rate=0.1, n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestRegressor(max_depth=10, n_estimators=300, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "# ✅ KFold 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "roc_auc_scores = []  # 각 Fold별 ROC-AUC 점수 저장\n",
    "\n",
    "test_predictions = {name: np.zeros(X_test.shape[0]) for name in models2.keys()}\n",
    "submission_predictions = {name: np.zeros(test.shape[0]) for name in models2.keys()}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_resampled)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "    X_train_fold, X_val_fold = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n",
    "    Y_train_fold, Y_val_fold = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n",
    "\n",
    "    for name, model in models2.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        model.fit(X_train_fold, Y_train_fold)\n",
    "        \n",
    "        # 검증 세트 예측\n",
    "        Y_val_pred = model.predict(X_val_fold)\n",
    "        \n",
    "        # ROC-AUC 점수 계산 및 저장\n",
    "        roc_auc = roc_auc_score(Y_val_fold, Y_val_pred)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        print(f\"📊 ROC-AUC (Fold {fold+1}, {name}): {roc_auc:.4f}\")\n",
    "        \n",
    "        # ✅ X_test 예측값 저장 (Soft Voting을 위해 확률 예측값 사용)\n",
    "        test_predictions[name] += model.predict(X_test) / kf.n_splits  \n",
    "        submission_predictions[name] += model.predict(test) / kf.n_splits  \n",
    "\"\"\"\n",
    "# 테스트 데이터에 대해서만 앙상블\n",
    "ensemble_pred_proba_test = (\n",
    "    test_predictions[\"LightGBM\"] * 0.4 +  # LightGBM 40%\n",
    "    test_predictions[\"XGBoost\"] * 0.3 +  # XGBoost 30%\n",
    "    test_predictions[\"CatBoost\"] * 0.3  # CatBoost 30%\n",
    ")\n",
    "\n",
    "# 제출 데이터에 대해서만 앙상블 (submission_predictions)\n",
    "ensemble_pred_proba_submission = (\n",
    "    submission_predictions[\"LightGBM\"] * 0.4 +\n",
    "    submission_predictions[\"XGBoost\"] * 0.3 +\n",
    "    submission_predictions[\"CatBoost\"] * 0.3\n",
    ")\n",
    "\n",
    "# ✅ 최적 Threshold 찾기\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ensemble_pred_proba_test)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[f1_scores[:-1].argmax()]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# ✅ 최적 Threshold 적용 후 예측값 변환\n",
    "Y_test_pred = (ensemble_pred_proba_test > optimal_threshold).astype(int)\n",
    "print(\"\\n=== Final Test Evaluation with Optimized Threshold ===\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fab58d8a-7f77-4dd2-9c6b-1b3ae2fbfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions[name] += model.predict(X_test) / kf.n_splits  \n",
    "submission_predictions[name] += model.predict(test) / kf.n_splits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3015661b-2502-4e8b-966e-1b51ca8d4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.8363\n",
      "\n",
      "=== Final Test Evaluation with Optimized Threshold ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85     38025\n",
      "           1       0.00      0.00      0.00     13246\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.37      0.50      0.43     51271\n",
      "weighted avg       0.55      0.74      0.63     51271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\3838496249.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = (2 * precision * recall) / (precision + recall)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대해서만 앙상블\n",
    "ensemble_pred_proba_test = (\n",
    "    test_predictions[\"LightGBM\"] * 0.4 +  # LightGBM 40%\n",
    "    test_predictions[\"XGBoost\"] * 0.3 +  # XGBoost 30%\n",
    "    test_predictions[\"CatBoost\"] * 0.3  # CatBoost 30%\n",
    ")\n",
    "\n",
    "# 제출 데이터에 대해서만 앙상블 (submission_predictions)\n",
    "ensemble_pred_proba_submission = (\n",
    "    submission_predictions[\"LightGBM\"] * 0.4 +\n",
    "    submission_predictions[\"XGBoost\"] * 0.3 +\n",
    "    submission_predictions[\"CatBoost\"] * 0.3\n",
    ")\n",
    "\n",
    "# ✅ 최적 Threshold 찾기\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ensemble_pred_proba_test)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[f1_scores[:-1].argmax()]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# ✅ 최적 Threshold 적용 후 예측값 변환\n",
    "Y_test_pred = (ensemble_pred_proba_test > optimal_threshold).astype(int)\n",
    "print(\"\\n=== Final Test Evaluation with Optimized Threshold ===\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b29667b5-36c8-4764-bfab-f192a6dd3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['probability'] = ensemble_pred_proba_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c33df816-e203-4cfa-adbc-4535068a3431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.010456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.001083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.368163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.188109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.534015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  probability\n",
       "0  TEST_00000     0.010456\n",
       "1  TEST_00001     0.001083\n",
       "2  TEST_00002     0.368163\n",
       "3  TEST_00003     0.188109\n",
       "4  TEST_00004     0.534015"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6d0d2c6-64ad-4e17-b591-e1d658c47395",
   "metadata": {},
   "outputs": [],
   "source": [
    " a.to_csv('prediction6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4ede3699-7b80-4f45-a185-7fc436084841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.8363\n",
      "\n",
      "=== Final Test Evaluation with Optimized Threshold ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85     38025\n",
      "           1       0.00      0.00      0.00     13246\n",
      "\n",
      "    accuracy                           0.74     51271\n",
      "   macro avg       0.37      0.50      0.43     51271\n",
      "weighted avg       0.55      0.74      0.63     51271\n",
      "\n",
      "\n",
      "=== Final Submission Evaluation with Optimized Threshold ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeongbin\\AppData\\Local\\Temp\\ipykernel_15940\\591525841.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = (2 * precision * recall) / (precision + recall)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [51271, 90067]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m Y_submission_pred \u001b[38;5;241m=\u001b[39m (ensemble_pred_proba_submission \u001b[38;5;241m>\u001b[39m optimal_threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Final Submission Evaluation with Optimized Threshold ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_submission_pred\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2671\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m \n\u001b[0;32m   2565\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2667\u001b[0m \u001b[38;5;124;03m<BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2670\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m-> 2671\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2674\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\projects\\lg_aimers_project\\venv_310\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [51271, 90067]"
     ]
    }
   ],
   "source": [
    "# 앙상블 예측값 계산 (test_predictions는 X_test에 대한 예측값)\n",
    "ensemble_pred_proba_test = (\n",
    "    test_predictions[\"LightGBM\"] * 0.4 +  # LightGBM 40%\n",
    "    test_predictions[\"XGBoost\"] * 0.3 +  # XGBoost 30%\n",
    "    test_predictions[\"CatBoost\"] * 0.3  # CatBoost 30%\n",
    ")\n",
    "\n",
    "# Y_test와 샘플 수가 일치하도록 ensemble_pred_proba_test 자르기\n",
    "ensemble_pred_proba_test = ensemble_pred_proba_test[:len(Y_test)]  # Y_test의 길이에 맞게 자름\n",
    "\n",
    "# 제출 데이터에 대한 앙상블 예측값 계산 (submission_predictions는 제출용 데이터 예측값)\n",
    "ensemble_pred_proba_submission = (\n",
    "    submission_predictions[\"LightGBM\"] * 0.4 +\n",
    "    submission_predictions[\"XGBoost\"] * 0.3 +\n",
    "    submission_predictions[\"CatBoost\"] * 0.3\n",
    ")\n",
    "\n",
    "# 최적 Threshold 찾기\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, ensemble_pred_proba_test)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[f1_scores[:-1].argmax()]\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "\n",
    "# 최적 Threshold 적용 후 예측값 변환\n",
    "Y_test_pred = (ensemble_pred_proba_test > optimal_threshold).astype(int)\n",
    "print(\"\\n=== Final Test Evaluation with Optimized Threshold ===\")\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "# 최종 제출 예측값 (보정된 앙상블 예측)\n",
    "Y_submission_pred = (ensemble_pred_proba_submission > optimal_threshold).astype(int)\n",
    "print(\"\\n=== Final Submission Evaluation with Optimized Threshold ===\")\n",
    "print(classification_report(Y_test, Y_submission_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23911f-700d-425f-9272-bedd52ace38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv_310)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
